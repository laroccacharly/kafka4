FROM apache/spark:3.5.0-scala2.12-java11-python3-ubuntu

WORKDIR /opt/spark/work-dir

USER root

# Install Maven and librdkafka
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    maven \
    librdkafka-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Copy POM file and download dependencies
COPY pom.xml .
RUN mvn dependency:copy-dependencies -DoutputDirectory=$SPARK_HOME/jars/

# Copy requirements and install Python dependencies
COPY requirements_consumer.txt .
RUN pip3 install --no-cache-dir -r requirements_consumer.txt

# Create necessary directories
RUN mkdir -p /app/data /app/checkpoints && \
    chown -R spark:spark /app

# Copy source code
COPY --chown=spark:spark . /app

# Set environment variables
ENV PYTHONPATH=/app:/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip
ENV PYSPARK_DRIVER_MEMORY=1g
ENV PYSPARK_EXECUTOR_MEMORY=1g

USER spark
WORKDIR /app

CMD ["python3", "consume.py"]